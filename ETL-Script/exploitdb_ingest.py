
"""
exploitdb_extract.py

Fetch Exploit-DB metadata from the official GitLab mirror (CSV or CSV.gz),
parse entries and write them to a JSONL file (one JSON object per line).


"""
import argparse
import csv
import gzip
import io
import json
import sys
import time
from typing import List, Dict, Any, Optional
import requests
import os

# Updated to use the official Exploit-DB GitLab mirror (more reliable than raw GitHub URLs)
RAW_URLS = [
    "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv",
    "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv.gz",
]

REQUEST_TIMEOUT = 30
RETRIES = 3
RETRY_DELAY = 4  # seconds


def fetch_text_from_url(url: str, verbose: bool = False) -> Optional[str]:
    for attempt in range(1, RETRIES + 1):
        try:
            if verbose:
                print(f"[INFO] GET {url} (attempt {attempt})")
            resp = requests.get(url, timeout=REQUEST_TIMEOUT)
            if resp.status_code != 200:
                if verbose:
                    print(f"[WARN] status {resp.status_code} for {url}")
                time.sleep(RETRY_DELAY)
                continue

            content = resp.content
            # detect gz by magic bytes
            if len(content) >= 2 and content[:2] == b'\x1f\x8b':
                if verbose:
                    print("[INFO] gzipped content detected; decompressing")
                try:
                    text = gzip.decompress(content).decode("utf-8", errors="replace")
                    return text
                except Exception as e:
                    if verbose:
                        print(f"[WARN] failed to decompress gz content: {e}")
                    # fallthrough to try text decode
            # otherwise try to decode as text
            try:
                return content.decode("utf-8", errors="replace")
            except Exception:
                return resp.text
        except Exception as e:
            if verbose:
                print(f"[WARN] error fetching {url}: {e}")
            time.sleep(RETRY_DELAY)
    return None


def read_local_file(path: str, verbose: bool = False) -> Optional[str]:
    if not os.path.exists(path):
        if verbose:
            print(f"[WARN] local file not found: {path}")
        return None
    try:
        with open(path, "rb") as fh:
            content = fh.read()
            # detect gz header
            if len(content) >= 2 and content[:2] == b'\x1f\x8b':
                if verbose:
                    print("[INFO] local gzipped file detected; decompressing")
                return gzip.decompress(content).decode("utf-8", errors="replace")
            else:
                try:
                    return content.decode("utf-8", errors="replace")
                except Exception:
                    # fallback
                    return content.decode("latin-1", errors="replace")
    except Exception as e:
        if verbose:
            print(f"[WARN] failed reading local file {path}: {e}")
        return None


def detect_header_and_parse(csv_text: str, verbose: bool = False) -> List[Dict[str, str]]:
    """
    Tries to parse CSV text. Detects header if present; otherwise assumes standard ExploitDB schema:
      id,file,description,date,author,platform,type
    Returns list of dicts keyed by header names.
    """
    f = io.StringIO(csv_text)
    reader = csv.reader(f)
    rows = list(reader)
    if not rows:
        return []
    # Detect header row: if first row contains non-numeric first column or clearly column names
    first = rows[0]
    header = None
    data_rows = []
    if any(cell.isalpha() for cell in first) or any("description" in c.lower() or "file" in c.lower() for c in first):
        header = [h.strip() for h in first]
        data_rows = rows[1:]
        if verbose:
            print(f"[INFO] Detected header with columns: {header}")
    else:
        # fallback to known schema
        header = ["id", "file", "description", "date", "author", "platform", "type"]
        data_rows = rows
        if verbose:
            print("[INFO] No header detected; using default schema:", header)

    parsed = []
    for r in data_rows:
        if not any(cell.strip() for cell in r):
            continue
        # pad/trim to header length
        if len(r) < len(header):
            r = r + [""] * (len(header) - len(r))
        rec = {header[i]: r[i].strip() for i in range(len(header))}
        parsed.append(rec)
    return parsed


def map_to_canonical(row: Dict[str, str]) -> Dict[str, Any]:
    """
    Minimal mapping from ExploitDB row to canonical schema fields (snake_case).
    This is intentionally small; expand as required.
    """
    src_id = row.get("id") or row.get("exploit_id") or ""
    title = row.get("description") or row.get("file") or ""
    published = row.get("date") or None

    canonical = {
        "source": "exploitdb",
        "source_id": f"exploitdb:{src_id}",
        "record_type": "exploit",
        "title": title,
        "short_description": row.get("description", ""),
        "full_description": row.get("description", ""),
        "cve_ids": [],  # not available directly in CSV; may require further lookup
        "affected_products": [],
        "cvss": None,
        "epss_score": None,
        "exploit_available": True,
        "indicators": [],
        "references": [],  # could build a URL using file or id if desired
        "tags": [row.get("platform", ""), row.get("type", "")],
        "published_date": published,
        "modified_date": None,
        "observed_at": None,
        "telemetry_metadata": None,
        "raw": {"row": row},
    }
    # attempt to create a reference URL if file or id present (best-effort)
    if row.get("file"):
        canonical["references"].append(row.get("file"))
    if src_id:
        canonical["references"].append(f"https://www.exploit-db.com/exploits/{src_id}")

    # add ingest metadata
    import datetime
    canonical["ingested_at"] = datetime.datetime.utcnow().isoformat() + "Z"
    canonical["ingested_by"] = "exploitdb_extract.py"
    return canonical


def write_jsonl(records: List[Dict[str, Any]], output_path: str, append: bool = True):
    mode = "a" if append else "w"
    with open(output_path, mode, encoding="utf-8") as fh:
        for rec in records:
            fh.write(json.dumps(rec, ensure_ascii=False) + "\n")


def main(argv=None):
    parser = argparse.ArgumentParser(description="Exploit-DB extractor - outputs JSONL")
    parser.add_argument("--output", "-o", default="exploitdb.jsonl", help="Output JSONL file")
    parser.add_argument("--limit", "-n", type=int, default=0, help="Limit number of records (0 = no limit)")
    parser.add_argument("--map-canonical", action="store_true", help="Map rows to minimal canonical schema")
    parser.add_argument("--verbose", action="store_true", help="Verbose logging")
    parser.add_argument("--local-file", "-l", default=None, help="Use local CSV file instead of downloading")
    args = parser.parse_args(argv)

    csv_text = None

    # local file fallback (if provided)
    if args.local_file:
        if args.verbose:
            print(f"[INFO] Attempting to read local file: {args.local_file}")
        csv_text = read_local_file(args.local_file, verbose=args.verbose)
        if not csv_text:
            print(f"[WARN] Could not read local file {args.local_file}. Will try remote URLs.")

    # try remote URLs if local not provided or failed
    if not csv_text:
        for url in RAW_URLS:
            csv_text = fetch_text_from_url(url, verbose=args.verbose)
            if csv_text:
                if args.verbose:
                    print(f"[INFO] fetched content from {url} (len={len(csv_text)})")
                break

    if not csv_text:
        print("[ERROR] Could not fetch Exploit-DB CSV from known mirrors or local file. Please check URLs or provide a local file using --local-file.")
        sys.exit(1)

    rows = detect_header_and_parse(csv_text, verbose=args.verbose)
    if args.limit and args.limit > 0:
        rows = rows[: args.limit]

    if args.verbose:
        print(f"[INFO] Parsed {len(rows)} rows")

    out_records: List[Dict[str, Any]] = []
    for row in rows:
        if args.map_canonical:
            out = map_to_canonical(row)
        else:
            out = {"source": "exploitdb", "source_id": row.get("id", ""), "raw_row": row}
        out_records.append(out)

    # write in append mode (so you can run incrementally); delete file beforehand if needed
    write_jsonl(out_records, args.output)
    print(f"[INFO] Wrote {len(out_records)} records to {args.output}")


if __name__ == "__main__":
    main()
