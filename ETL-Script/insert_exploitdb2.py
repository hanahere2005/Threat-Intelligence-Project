import json
import psycopg2
from psycopg2.extras import execute_batch
from datetime import datetime

# ===============================
# 🔐 Database Config
# ===============================
DB_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "dbname": "Threat-Intelligence-Database-Schema",
    "user": "postgres",
    "password": "root@123"
}

# ===============================
# 📄 File path
# ===============================
JSONL_FILE = "exploitdb2.jsonl"

# ===============================
# 🧭 Insert query
# ===============================
INSERT_QUERY = """
INSERT INTO "Storing-Threat-Data".cves (
    external_id,
    title,
    description,
    category,
    severity,
    cvss_score,
    cvss_vector,
    cwe_list,
    cwe_ids,
    vendors,
    products,
    affected_products_count,
    references_count,
    "references",
    tags,
    original_source_url,
    actual_url,
    source_name,
    published_date,
    last_updated_from_source,
    ingested_at,
    data_version,
    metadata,
    source
) VALUES (
    %(external_id)s,
    %(title)s,
    %(description)s,
    %(category)s,
    %(severity)s,
    %(cvss_score)s,
    %(cvss_vector)s,
    %(cwe_list)s,
    %(cwe_ids)s,
    %(vendors)s,
    %(products)s,
    %(affected_products_count)s,
    %(references_count)s,
    %(references)s,
    %(tags)s,
    %(original_source_url)s,
    %(actual_url)s,
    %(source_name)s,
    %(published_date)s,
    %(last_updated_from_source)s,
    %(ingested_at)s,
    %(data_version)s,
    %(metadata)s,
    %(source)s
)
ON CONFLICT (external_id) DO NOTHING;
"""

# ===============================
# 🧾 Helper function to clean & map data
# ===============================
def parse_record(line):
    obj = json.loads(line)
    raw = obj.get("raw_row", {})

    # Extract CVE or unique identifier from codes field
    external_id = None
    codes = raw.get("codes", "")
    if codes:
        parts = codes.split(";")
        for p in parts:
            if p.startswith("CVE-"):
                external_id = p.strip()
                break
    if not external_id:
        external_id = f"{obj['source']}-{obj['source_id']}"

    # Tags as array
    tags = [t.strip() for t in raw.get("tags", "").split(",") if t.strip()]

    # CWE or similar IDs
    cwe_list = [c.strip() for c in codes.split(";") if c.strip()] if codes else None

    # Dates
    def parse_date(date_str):
        if not date_str:
            return None
        try:
            return datetime.strptime(date_str, "%Y-%m-%d")
        except Exception:
            return None

    published_date = parse_date(raw.get("date_published"))
    updated_date = parse_date(raw.get("date_updated"))

    # Prepare row dictionary
    return {
        "external_id": external_id,
        "title": raw.get("description", "")[:255],
        "description": raw.get("description", ""),
        "category": raw.get("type"),
        "severity": None,  # No severity in exploitdb directly
        "cvss_score": None,
        "cvss_vector": None,
        "cwe_list": cwe_list,
        "cwe_ids": cwe_list,
        "vendors": [raw.get("author")] if raw.get("author") else None,
        "products": [raw.get("platform")] if raw.get("platform") else None,
        "affected_products_count": 1 if raw.get("platform") else None,
        "references_count": 1 if raw.get("source_url") else None,
        "references": json.dumps({"source_url": raw.get("source_url")}),
        "tags": tags,
        "original_source_url": raw.get("source_url"),
        "actual_url": raw.get("source_url"),
        "source_name": obj.get("source"),
        "published_date": published_date,
        "last_updated_from_source": updated_date,
        "ingested_at": datetime.utcnow(),
        "data_version": None,
        "metadata": json.dumps(raw),
        "source": obj.get("source")
    }

# ===============================
# 🚀 Main function
# ===============================
def main():
    print("[*] Connecting to database...")
    conn = psycopg2.connect(**DB_CONFIG)
    cursor = conn.cursor()

    batch = []
    batch_size = 1000
    total_inserted = 0

    with open(JSONL_FILE, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            record = parse_record(line)
            batch.append(record)

            if len(batch) >= batch_size:
                execute_batch(cursor, INSERT_QUERY, batch)
                conn.commit()
                total_inserted += len(batch)
                print(f"[+] Inserted {total_inserted} records so far...")
                batch = []

    # Insert remaining
    if batch:
        execute_batch(cursor, INSERT_QUERY, batch)
        conn.commit()
        total_inserted += len(batch)

    print(f"[✅] Finished inserting {total_inserted} records.")
    cursor.close()
    conn.close()

if __name__ == "__main__":
    main()
